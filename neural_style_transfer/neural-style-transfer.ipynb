{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bc44fb-faa1-433f-9043-2f180fd46209",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "Resources:   \n",
    "https://towardsdatascience.com/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfa  \n",
    "https://pytorch.org/tutorials/advanced/neural_style_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08ab1cf-ed0b-40b6-825f-2ff8f80d20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c9ab2-8e1d-42e9-bfff-50929ab16d35",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Here we describe vgg19 model.\n",
    "\n",
    "Some image of vgg19.\n",
    "\n",
    "Then its implementation in pytorch and how I am slicing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07fc01-50e5-42ae-810d-4567b5eca218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.model = models.vgg19(pretrained=True)\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664e504-063d-43c8-814d-410d691e3fd4",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c390769-fc07-4175-a918-fd023a199232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047cb19a-5a33-400f-83ef-3786c7805232",
   "metadata": {},
   "source": [
    "### Total Loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{total}(\\vec{p}, \\vec{a}, \\vec{x}) = \\alpha\\mathcal{L}_{content}(\\vec{p}, \\vec{x}) + \\beta\\mathcal{L}_{style}(\\vec{a}, \\vec{x})\n",
    "$$\n",
    "$ \\vec{p} $ - content image  \n",
    "$ \\vec{a} $ - style image   \n",
    "$ \\vec{x} $ - generated image   \n",
    "$ \\alpha $ - content coefficient   \n",
    "$ \\beta $ - style cooefficient \n",
    "\n",
    "> Generated image $\\vec{x}$ can be either initialized as content image or white noise (random values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4200f4-e33b-4057-ac81-11f19f3a2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalLoss(nn.Module):\n",
    "    def __init__(self, content_features: Tensor, style_features: Tensor, alpha: float = 1., beta: float = 1000.):\n",
    "        super(TotalLoss, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.content_loss = ContentLoss(content_features)\n",
    "        self.style_loss = StyleLoss(style_features)\n",
    "\n",
    "    def forward(self):\n",
    "        total_loss = self.alpha * self.content_loss + self.beta * self.style_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f02e16-6423-4fc1-abd7-a399082ff7ca",
   "metadata": {},
   "source": [
    "### Content Loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{content}(\\vec{p}, \\vec{x}) = \\dfrac{1}{2} \\sum_{i,j}(F_{i,j}^{l} - P_{i,j}^{l})^2\n",
    "$$\n",
    "\n",
    "Content loss is a mean squared error between: \n",
    "  \n",
    "$ F_{i,j}^{l} $ - Output of conv layer **$l$** for input (generated) image  \n",
    "$ P_{i,j}^{l} $ - Output of conv layer **$l$** for content image  \n",
    "\n",
    "> $ i, j $ represent **i-th** position of the filter at position **j** which implementation-wise doesn't change anything as we take whole outputs of conv layers\n",
    "\n",
    "There's a small difference in notation when it comes to implementation. In paper, $F^{l}$ is defined as $F^{l} \\in \\mathbb{R}^{N_l x M_l}$, which means it's a matrix with shape ($N_l$ - number of feature maps, $M_l$ - $width * height$ of feature maps). In contrast, here we have $F^{l} \\in \\mathbb{R}^{N_l x H_l x W_l}$ which is just a reshaped version of the matrix in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93031e9b-9174-416a-871f-c0e27df93abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, content_features: Tensor):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.content_features = content_features\n",
    "\n",
    "    def forward(self, input_features: Tensor) -> Tensor:\n",
    "        return self.mse(input_features, self.content_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452459e-ebf7-4163-aaf6-f2229c232b93",
   "metadata": {},
   "source": [
    "### Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e2226-88ec-4d2a-9708-aa85f1836320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
